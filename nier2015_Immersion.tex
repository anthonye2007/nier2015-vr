
\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{subcaption}

\begin{document}

\title{Consumer Virtual Reality in Software Engineering: Affordances, Applications, and Challenges}

\author{\IEEEauthorblockN{Anthony Elliott}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, North Carolina, USA\\
anthony\_elliott@ncsu.edu}
\and
\IEEEauthorblockN{Brian Peiris}
\IEEEauthorblockA{Toronto, Ontario, Canada\\
brianpeiris@gmail.com}
\and
\IEEEauthorblockN{Chris Parnin}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, North Carolina, USA\\
chris.parnin@ncsu.edu}}

\maketitle
\begin{abstract}
Virtual reality (VR) has recently been used to great effect by the gaming industry and offers orders of magnitude of improvement in software engineering in the years to come.
A VR environment completely surrounds and immerses the user and uses stereoscopic rendering to enable depth-sensing. This enables the user to process more information and in a more natural way than traditional environments on a monitor.

We describe potential VR environments for code review, live coding, remote collaboration, and simulation.
We also consider open research questions in this area.
\end{abstract}

\section{Introduction}
Quality virtual reality (VR) is finally inexpensive enough for consumers, including software engineers, to invest in. 
No longer will we have to go to a physical room to be fully immersed in a virtual reality, within the next few years we will be able to simply reach over and put on our fully immersive VR sunglasses and headphones. 
Virtual reality is often applied only to the entertainment industry but we argue that it will greatly change software engineering as well. 

VR devices render a slightly different image for each eye (stereoscopic rendering) which allows humans to sense depth in the images just like in the physical world. 
VR devices also completely immerse the user in the environment, so that the user's head has six degrees of freedom. 
This realistic imitation of the human body enables easier and more natural navigation as well as the ability to display more information at once in a meaningful way. 
We get the naturalness of the physical world and the malleabillity of the virtual world.

We believe that these affordances provide a platform for new kinds of software engineering tools. 
We imagine a future in which many software engineers find it most productive to design and build software within a VR environment. 

To immerse the user in a virtual environment we used a head-mounted display (Oculus Rift - Development Kit 2) and a Leap Motion Controller for gesture recognition. 
These devices are inexpensive (total is less than \$500) and future iterations of them could feasibly be used by a number of developers within five years.

\begin{figure}[ht!]
\centering
\begin{subfigure}{.5\columnwidth}
	\centering
	\includegraphics[width=.4\linewidth]{figures/dk2}
	\caption{Oculus Rift\label{fig:rift}}
\end{subfigure}%
\begin{subfigure}{.5\columnwidth}
  \centering
  \includegraphics[width=.4\linewidth]{figures/LeapAndKeyboard}
  \caption{Leap Motion Controller \label{fig:leap}}
\end{subfigure}
\end{figure}

There have been substantial amounts of VR research done in the past, but very little of it has shown how VR can aid in software engineering. 
In this paper we present a vision of four software engineering activities that could benefit from VR.

We illustrate how immersive 3D environments could make software engineering tools easier to learn and use, and to display more information without overwhelming users. 
We show that such environments enable much tighter feedback loops in development which helps software engineers develop faster and with fewer mistakes. 
We describe shared virtual environments that could increase communication among remote teams. 
We also illustrate how virtual reality can simulate fields that have physical analogs.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth,height=5cm]{figures/unwarpped_stack}
\caption{The active fragment is in the middle of the user's view. The user is currently looking at the implementations of the methods (shown to the right) called by the active fragment. The user is able to easily read 3 of these fragments at once.  \label{immersion}}
\end{figure*}

\section{Related Work}
Virtual environments called CAVEs have been around since the 1990's in which a person can be in a physical room with displays covering all surfaces~\cite{Cruz-Neira:CAVE}.  
Users often use head-mounted displays with head tracking to update the displays and may use hand-held devices to enable interaction with objects in the CAVE.

Andrew Bragdon has implemented a system called Code Bubbles that enables the user to pull out methods from a file. 
The user is then able to move and group the methods as desired.  
Code Bubbles also can display the call graph for the selected method and allows the user to open called methods~\cite{Bragdon:CodeBubbles}.

Code Canvas added semantic zoom to the Code Bubbles tool, allowing the user to gain a better understanding of how the current section of the system fits into the system as a whole~\cite{DeLine:CodeCanvas}.

Chris Parnin developed a tool called NosePrints which allows users to quickly see if a code smell is relevant~\cite{parnin:Noseprints}.

Agarwala used piles to add structure to a 3D desktop.~\cite{Agarawala:BumpTop}

Yongpisanpop created Bugarium that integrated a Leap Motion Controller to aid both interaction and visualization in a large-scale bug repository~\cite{Bugarium}.

RoomAlive by Microsoft Research is similar to a CAVE but uses six Kinect sensors around the room to track the user.  This eliminates the need for wearable devices by the user but requires significant amounts of space.~\cite{Jones:RoomAlive}

\section{Applications}
% ties all application together
% take advantage of the affordances of VR
  % immersive env => lots of info at once
  % stereoscopic => true 3D; depth
Virtual environments can use the immersive property to display more information than on a computer monitor. 
We are still limited by the field of view of the human eye, so putting more information on the display is not novel, but what is novel is being able to distribute that information around the user in a meaningful way. 
This immersion is possible because the Rift uses stereoscopic rendering (creates a slightly different image for each eye) to enable depth sensing.

In this section we describe how VR could aid in code review, live coding, remote collaboration, and simulation. 
Our main contribution is not these specific ideas, but to show the possibilities of software engineering with VR.

\subsection{Code Review}
The primary reason for modern code reviews not resulting in more defects found is a lack of understanding of the code being modified~\cite{bacchelli:ModernCodeReviewChallenges}. 
Immersion is a prototype of a VR tool that seeks to use the immersive 3D environment to enable reviewers to gain a better understanding of the code being modified. 
The user views the environment through an Oculus Rift and interacts through hand gestures recognized by a Leap Motion Controller.

Immersion shows code fragments similar to Code Bubbles~\cite{Bragdon:CodeBubbles}, but each fragment consists of exactly one method.  
The reviewer initially sees the active fragment in the center of the screen with other relevant information distributed around the floor in piles with more relevant piles closer to the reviewer. 
The reviewer is able to scan the label of the pile and number of fragments in each pile to quickly verify if this pile is indeed relevant. 
The reviewer then makes a grabbing motion towards the most interesting pile and pulls their hand up to show all of the fragments in a circular expanded view to the right of the active fragment.

The reviewer is now able to read the foremost fragment in the circle and can make horizontal finger swipes to rotate the circle and read other fragments. 
Immersion uses semantic zoom to make method names readable even when at the back of the circle and the fragment retains its height so that the reviewer can quickly see how many lines of code it has. 
The reviewer can select an especially large method using a pinch motion on the top and bottom and move it to become the active fragment. 
The piles then change to include code relevant to the new fragment. 
The reviewer can move their hand as if clearing off a desk to return to the previous active fragment.

Natural interaction paradigms are not new, but are even more powerful in a immersive 3D environment where the user truly feels like they are \textit{inside} the environment. 
This feeling of presence could make immersive VR tools easier to use when compared to a 3D tool displayed on computer screen. 

Immersion also displays overview diagrams of this changeset to allow the reviewer to see where this active fragment fits into the changeset as a whole. 
Similarly an overview of the cyclomatic complexity also helps the reviewer to see which areas of the code would be most relevant to see. 
These diagrams illustrate how VR is able to show multiple overview and detail diagrams without overwhelming the user.

Immersion is merely a vehicle to demonstrate the advantages that VR can bring to software engineering. 
Most software engineering tools don't display information in 3D because its not needed and merely looks cute. 
An immersive 3D environment enables more information to be displayed which should increase developer productivity.
Additionally, such a VR environment allows users to interact more naturally with the tool which would make VR tools easier to learn and use.

\subsection{Live Coding}
RiftSketch is a live coding environment built for VR. 
It allows a user to describe a 3D scene using geometries, materials, textures, lights, meshes and other primitives provided by the Three.js library, which is a convenient wrapper for the WebGL APIs available in modern browsers. 
RiftSketch presents a user with a simple text editor, floating in front of them in an otherwise empty VR world. 
As the user types code into the editor, the world around them updates instantly to display the 3D scene dictated by their code. 
RiftSketch also allows the user to animate their scene via a callback function which is executed on every frame. 
The user can manipulate the state of the 3D scene in this looped block of code in order to add behaviour to the objects in their scene.

\begin{figure}[ht!]
\centering
\includegraphics[width=80mm]{figures/riftsketch/closer}
\caption{RiftSketch screenshot \label{riftsketch}}
\end{figure}

Furthermore, RiftSketch provides the user with shortcuts and input methods to quickly edit numbers in the code that they write. 
Keyboard shortcuts allow the user to increment or decrement numbers in the code by increments of 0.1, 10 or 100. 
Integration with the Leap Motion Controller provides users with the ability to manipulate numbers using natural input: the user places the editor's cursor over a number and then can move their hand up and down above the Leap Motion controller in order to increase or decrease the number. 
Numbers in the code could represent anything, from the X,Y or Z components of a position or rotation vector, the red, green or blue components of an object's material or a component in the calculation of an object's animated speed.

Although the current implementation of RiftSketch is little more than a creative toy, the experience of using it shows the potential of practical applications. 
Largely inspired by Bret Victor's seminal lecture, "Inventing on Principle"~\cite{Victor:InventingOnPrincipleVideo}\cite{Victor:InventingOnPrincipleTranscript}, RiftSketch demonstrates the potency of giving a creator, or programmer, a direct and immediate connection to their creation. 
This tight feedback loop allows the programmer to better understand the effect of every part of the code that they write and to quickly experiment with various solutions, algorithms and calculations. 
RiftSketch is also very effective as a learning tool since users can see their mistakes immediately and correct themselves without an intermediate compile step that might otherwise act as a roadblock.  
These benefits are especially evident in RiftSketch when the code describes a VR scene. 
Watching the entire virtual world change around you as you type can be an extremely powerful and engaging experience.

In practical applications, one could extend the concept of live coding and tight feedback loops in order to create customized programming environments. 
VR programming environments could allow the user to take advantage of the malleability and expanse of virtual worlds. 
For example, a user could live code a widget that indicates some statistic of the code they are currently editing, such as the cyclomatic complexity, test coverage or test result. 
The widget would update itself as the user typed, not unlike existing code feedback tools such as NCrunch \footnote{NCrunch automated concurrent testing tool: http://www.ncrunch.net/}, and they could position the widget in their virtual periphery.

\subsection{Remote Collaboration}
% Imagining room inside HiFi with multiple avatars
% each developer could have an avatar
% able to get facial expressions
% able to get body language (although somewhat expensive right now, these prices will come down)

% able to sense body language in real time
% able to hear in real time
% environment is dynamic

% pair programming
Co-located development teams are able to communicate in person. 
Distributed teams can try, but the technology is too slow and it doesn't feel good. 
New tech is coming (High Fidelity) that is fast enough for developers to be able to sense body language and hear audio in real time.  
Distributed development teams will have the foundational issues of remote communication solved which enables a shared VR environment to become truly powerful.

A benefit of co-location is pair programming, where two developers program at the same computer and can catch each others errors. 
VR and body sensing with a device like PrioVR would enable remote developers to be at a shared virtual desk right next to each other. 
They would be able to see the same files, talk in real time, and even point to which line of the code they are talking about. 

% pair 'live coding'/code review
Sharing a live coding environment enables all participants to have a tight feedback loop. 
This could be especially useful when the primary developer of a section needs to explain their modifications to the rest of the team. 
Everyone could meet in the shared VR live coding environment and watch while the developer walks through modification of their code. 
The team would be able to see how the system changed rather than looking through large amounts of code.

  % pair designing
Co-located teams can design together around a whiteboard in a conference room as they talk through the problems of the system. 
A shared VR environment could have a similar room with sketching space that everyone can see. 
Developers could drag-and-drop files or code snippets to display on the walls around them. 
They could then discuss and annotate the code. 
Additionally, visualizations such as structure diagrams or architecture of related systems could be overlaid on a wall for reference.

% exploring data visualization together
Remote workers could experience immersive visualizations together. 
A control flow graph could be represented by a maze of rooms through which the team could walk. 
The team members could trace different paths and yet still be shown an overlay of where each of the other members are and how these positions relate to each other. 
Currently remote workers can each trace separate paths of the code but have a hard time communicating what those paths are and how they relate to each other.

Remote workers often miss out on many benefits that in-person communication can bring. 
Shared VR spaces tailored to developers' activities can fill some of the gaps caused by distance and help the team to function better together. 

\subsection{Simulation}
VR worlds are also well-suited to applications with physical analogs that can be simulated in 3D. 
For example, if a programmer was working on avionics software in a live coding environment, the VR world could include a simulation of a plane with moving control surfaces, propellers or a visualization of the aircraft's fuel injection system. 
The live coding environment would then give the programmer immediate visual feedback about the control flow and logic that they were programming. 
The programmer could scale the virtual plane to a manageable size to give her an overview of its behaviour, scale it to normal size and walk around the plane to inspect a particular control surface or scale it to larger than life-size, transport herself into the guts of the plane to watch a particular fuel valve respond to the state of the program.

[include rest of Brian's draft here]

\section{Discussion}
% VR vs AR in the future, which is better?
  % code snippets while wearing Google Glass could appear on peripherals
We have presented virtual environments that completely immerse the user. 
Other devices such as Google Glass, aim to help the user complete tasks in the physical world by adding information overlays (augmented reality). 
Augmented reality seeks to help the user in the physical world while virtual reality seeks to completely replace physical reality. 
Is it more useful to immerse the user in a completely virtual, dynamic environment, or to enhance the physical world of developers?
  
% Manipulating environment
% Utilize the 'experience'?
  % Gamification of code review
Virtual reality is able to make users feel `presence', that is, to trick their mind into thinking they are physically in the environment. 
This is critical to entertainment experiences but would creating such a compelling experience have tangible benefits like improved defect rate in a code review environment? 

% Where is the limit for too much info displayed at once?
The main advantage of VR is being able to display more information at once to the user.  
At what point does the extra information cause overload and begin to hinder productivity?  
Is this balance between amount of information and processing speed the same for each person (other aspects of VR can differ per person)?

% Does using a VR code review tool actually increase comprehension? Does it allow users to find bugs and provide alternative solutions?

% What gestures are most natural in VR?
What is the best way for the user to provide input to a VR system? 
The games industry has found a console controller to work well for navigation and limited action support but those controllers are poorly suited for text entry. 
However, while the keyboard is well suited for text entry, many people find it hard to type while wearing a head-mounted display. 
Gesture recognition devices like the Leap Motion Controller and Kinect sensor enable the user to perform more natural gestures, but often put too much strain on the user's arms. 
Voice added to gesture recognition could help to reduce that arm strain, but speaking to one's computer may feel disruptive and awkward in a shared office space.

How can software engineers take advantage of the affordances of VR to improve upon existing 3D visualizations?

% What use the z-axis for?
% Use VR for specific activities?
% Will a developer buy a Rift just for the tools?
What barriers of adoption will be present for software engineers and what steps can researchers take now to mitigate those barriers?

\subsection{Challenges}
Here is a chance to include some honest things that need to be overcome, for example, resolution, nausea, configuration, pupil distance, etc.

\section{Conclusions}
We have shown that virtual reality enables a new interaction paradigm that can improve developers' lives. 
The immersive environment allows users to process more information at once and stereoscopic rendering enables depth-sensing. 
Together, these affordances provide a novel platform for new software engineering tools.

\bibliographystyle{abbrv}
\bibliography{nier2015_Immersion}
\end{document}
